{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f51d07d-7be1-4125-9070-e8e8d451cfc1",
   "metadata": {},
   "source": [
    "## Descripción del Código en Python\n",
    "\n",
    "Se empezará con el procesamiento de imágenes y la preparación de datos para un modelo de aprendizaje automático de reconocimiento de emociones.\n",
    "\n",
    "### Importación de Bibliotecas\n",
    "\n",
    "En esta sección, se importan las bibliotecas necesarias para el procesamiento de imágenes y la manipulación de datos. Las bibliotecas incluyen:\n",
    "\n",
    "- `os`: Para operaciones relacionadas con el sistema operativo.\n",
    "- `cv2` (OpenCV): Para el procesamiento de imágenes.\n",
    "- `numpy` (abreviado como `np`): Para la manipulación de arreglos numéricos multidimensionales.\n",
    "- `train_test_split` de `sklearn.model_selection`: Para dividir los datos en conjuntos de entrenamiento y validación.\n",
    "\n",
    "### Definición del Directorio de Datos\n",
    "\n",
    "Se establece el directorio que contiene las imágenes y etiquetas de entrenamiento en la variable `data_dir`.\n",
    "\n",
    "### Especificación del Tamaño de las Imágenes de Entrada\n",
    "\n",
    "Se define el tamaño deseado para las imágenes de entrada en la variable `img_size`.\n",
    "\n",
    "### Enumeración de Emociones Disponibles\n",
    "\n",
    "Se crea una lista llamada `emotions` que contiene las emociones disponibles como clases a predecir.\n",
    "\n",
    "### Lectura de Imágenes y Etiquetas\n",
    "\n",
    "El código realiza las siguientes acciones:\n",
    "\n",
    "- Itera sobre cada emoción en la lista `emotions`.\n",
    "- Para cada emoción, itera a través de los archivos en el directorio correspondiente.\n",
    "- Lee las imágenes en escala de grises (usando `cv2.imread`) y las redimensiona al tamaño especificado en `img_size`.\n",
    "- Las imágenes se almacenan en la lista `images`.\n",
    "- Las etiquetas se almacenan en la lista `labels` como el índice de la emoción correspondiente en la lista `emotions`.\n",
    "\n",
    "### Conversión de Listas a Arrays Numpy\n",
    "\n",
    "Las listas `images` y `labels` se convierten en arrays numpy para su posterior procesamiento. Además, las imágenes se normalizan dividiendo sus valores de píxeles por 255.0.\n",
    "\n",
    "### División de Datos en Conjuntos de Entrenamiento y Validación\n",
    "\n",
    "Los datos se dividen en conjuntos de entrenamiento (`train_images` y `train_labels`) y validación (`val_images` y `val_labels`) utilizando la función `train_test_split` de scikit-learn. El 20% de los datos se asigna al conjunto de validación, y el generador de números aleatorios se inicializa con una semilla de 42 para asegurar la reproducibilidad.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f979733-f0c5-4f5b-9fd6-43b935f7b39b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Directorio que contiene las imágenes de caras y las etiquetas\n",
    "data_dir = 'DataSet/images/images/train/'\n",
    "\n",
    "# Tamaño de las imágenes de entrada\n",
    "img_size = (48, 48)\n",
    "\n",
    "# Lista de emociones disponibles\n",
    "emotions = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n",
    "\n",
    "# Leer las imágenes y las etiquetas\n",
    "images = []\n",
    "labels = []\n",
    "for emotion in emotions:\n",
    "    emotion_dir = os.path.join(data_dir, emotion)\n",
    "    for filename in os.listdir(emotion_dir):\n",
    "        if filename.endswith('.jpg'):\n",
    "            img = cv2.imread(os.path.join(emotion_dir, filename), cv2.IMREAD_GRAYSCALE)\n",
    "            img = cv2.resize(img, img_size)\n",
    "            images.append(img)\n",
    "            labels.append(emotions.index(emotion))\n",
    "\n",
    "# Convertir las listas a arrays numpy\n",
    "images = np.array(images)\n",
    "images = images / 255.0\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y validación\n",
    "train_images, val_images, train_labels, val_labels = train_test_split(images, labels, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ae905c-14df-4d23-b360-223ebb0e5365",
   "metadata": {},
   "source": [
    "## Descripción del Modelo de Red Neuronal Convolucional (CNN)\n",
    "\n",
    "A continuación se compila un modelo de Red Neuronal Convolucional (CNN) utilizando las bibliotecas TensorFlow y Keras para el reconocimiento de emociones en imágenes. A continuación, se presenta una descripción formal de lo que hace el código:\n",
    "\n",
    "### Creación del Modelo CNN\n",
    "\n",
    "1. **Capa Convolucional 1**: Se añade una capa convolucional con 32 filtros y una función de activación ReLU. La entrada de la capa tiene el tamaño especificado en `img_size`. Esta capa realiza la detección de características en las imágenes.\n",
    "\n",
    "2. **Capa de Agrupación 1 (MaxPooling)**: Se añade una capa de agrupación (pooling) MaxPooling para reducir el tamaño de la representación y conservar las características más importantes.\n",
    "\n",
    "3. **Capa Convolucional 2**: Se agrega otra capa convolucional con 64 filtros y función de activación ReLU.\n",
    "\n",
    "4. **Capa de Agrupación 2 (MaxPooling)**: Se añade otra capa de agrupación MaxPooling.\n",
    "\n",
    "5. **Capa Convolucional 3**: Se agrega una tercera capa convolucional con 128 filtros y función de activación ReLU.\n",
    "\n",
    "6. **Capa de Agrupación 3 (MaxPooling)**: Se incluye una tercera capa de agrupación MaxPooling.\n",
    "\n",
    "7. **Capa de Aplanamiento (Flatten)**: La salida de la última capa de agrupación se aplana para prepararla para las capas completamente conectadas.\n",
    "\n",
    "8. **Capa Completamente Conectada 1**: Se añade una capa completamente conectada con 256 neuronas y función de activación ReLU.\n",
    "\n",
    "9. **Capa de Dropout**: Para evitar el sobreajuste, se agrega una capa de dropout que desactiva aleatoriamente un 50% de las neuronas durante el entrenamiento.\n",
    "\n",
    "10. **Capa Completamente Conectada 2 (Capa de Salida)**: La capa de salida tiene un número de neuronas igual a la cantidad de emociones disponibles. Utiliza la función de activación softmax para realizar la clasificación multiclase de las emociones.\n",
    "\n",
    "### Compilación del Modelo\n",
    "\n",
    "El modelo se compila utilizando las siguientes configuraciones:\n",
    "\n",
    "- **Función de Pérdida**: Se utiliza la función de pérdida 'sparse_categorical_crossentropy' para problemas de clasificación multiclase.\n",
    "\n",
    "- **Optimizador**: Se emplea el optimizador 'adam' para ajustar los pesos de la red durante el entrenamiento.\n",
    "\n",
    "- **Métrica de Evaluación**: La métrica de evaluación se establece como 'accuracy' para supervisar la precisión del modelo durante el entrenamiento.\n",
    "\n",
    "Este modelo de CNN se utilizaría para entrenar un clasificador de emociones basado en imágenes y se podría ajustar y entrenar utilizando los datos de entrenamiento y validación previamente preparados.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058e21e4-8c6c-489e-b1f6-ef0c7c8fa900",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense\n",
    "#import tf.keras.applications.resnet\n",
    "\n",
    "# Crear el modelo\n",
    "model = Sequential()\n",
    "\n",
    "# Capa convolucional\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(img_size[0], img_size[1], 1)))\n",
    "\n",
    "# Capa de agrupación\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Capa convolucional\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "\n",
    "# Capa de agrupación\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Capa convolucional\n",
    "model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "\n",
    "# Capa de agrupación\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Capa completamente\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(emotions), activation='softmax'))\n",
    "\n",
    "# Compilar el modelo\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a101965f-eead-4261-bce4-9b04a6a34353",
   "metadata": {},
   "source": [
    "## Entrenamiento del Modelo de Red Neuronal Convolucional (CNN)\n",
    "\n",
    "Se entrena modelo de Red Neuronal Convolucional (CNN) utilizando los datos de entrenamiento y validación que han sido preparados previamente.\n",
    "### Entrenamiento del Modelo\n",
    "\n",
    "1. `model.fit(train_images, train_labels, epochs=30, validation_data=(val_images, val_labels))`\n",
    "\n",
    "   - El método `fit` se utiliza para entrenar el modelo.\n",
    "   - `train_images` y `train_labels` son los datos de entrenamiento de las imágenes y las etiquetas asociadas.\n",
    "   - `epochs=30` indica que el modelo se entrenará durante 30 épocas completas. Una época es un ciclo a través de todo el conjunto de datos de entrenamiento.\n",
    "   - `validation_data=(val_images, val_labels)` especifica el conjunto de datos de validación que se utilizará para evaluar el rendimiento del modelo después de cada época de entrenamiento.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fc26b8-e72e-4f8f-8b51-4edf58975877",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "721/721 [==============================] - 37s 50ms/step - loss: 1.7146 - accuracy: 0.3060 - val_loss: 1.5429 - val_accuracy: 0.4031\n",
      "Epoch 2/20\n",
      "721/721 [==============================] - 40s 56ms/step - loss: 1.4839 - accuracy: 0.4301 - val_loss: 1.3832 - val_accuracy: 0.4647\n",
      "Epoch 3/20\n",
      "721/721 [==============================] - 48s 67ms/step - loss: 1.3645 - accuracy: 0.4761 - val_loss: 1.3208 - val_accuracy: 0.5018\n",
      "Epoch 4/20\n",
      "721/721 [==============================] - 48s 67ms/step - loss: 1.2772 - accuracy: 0.5120 - val_loss: 1.2819 - val_accuracy: 0.5096\n",
      "Epoch 5/20\n",
      "721/721 [==============================] - 48s 66ms/step - loss: 1.2190 - accuracy: 0.5366 - val_loss: 1.2356 - val_accuracy: 0.5289\n",
      "Epoch 6/20\n",
      "721/721 [==============================] - 50s 69ms/step - loss: 1.1641 - accuracy: 0.5556 - val_loss: 1.2194 - val_accuracy: 0.5388\n",
      "Epoch 7/20\n",
      "721/721 [==============================] - 53s 73ms/step - loss: 1.1111 - accuracy: 0.5789 - val_loss: 1.2146 - val_accuracy: 0.5365\n",
      "Epoch 8/20\n",
      "721/721 [==============================] - 49s 68ms/step - loss: 1.0670 - accuracy: 0.5934 - val_loss: 1.2019 - val_accuracy: 0.5428\n",
      "Epoch 9/20\n",
      "721/721 [==============================] - 49s 68ms/step - loss: 1.0303 - accuracy: 0.6105 - val_loss: 1.1961 - val_accuracy: 0.5598\n",
      "Epoch 10/20\n",
      "721/721 [==============================] - 48s 67ms/step - loss: 0.9797 - accuracy: 0.6296 - val_loss: 1.2359 - val_accuracy: 0.5350\n",
      "Epoch 11/20\n",
      "721/721 [==============================] - 68s 94ms/step - loss: 0.9406 - accuracy: 0.6484 - val_loss: 1.2167 - val_accuracy: 0.5544\n",
      "Epoch 12/20\n",
      "721/721 [==============================] - 73s 101ms/step - loss: 0.8996 - accuracy: 0.6610 - val_loss: 1.2567 - val_accuracy: 0.5539\n",
      "Epoch 13/20\n",
      "721/721 [==============================] - 52s 72ms/step - loss: 0.8493 - accuracy: 0.6764 - val_loss: 1.2729 - val_accuracy: 0.5509\n",
      "Epoch 14/20\n",
      "721/721 [==============================] - 53s 73ms/step - loss: 0.8040 - accuracy: 0.6972 - val_loss: 1.2778 - val_accuracy: 0.5540\n",
      "Epoch 15/20\n",
      "721/721 [==============================] - 51s 71ms/step - loss: 0.7727 - accuracy: 0.7055 - val_loss: 1.3020 - val_accuracy: 0.5605\n",
      "Epoch 16/20\n",
      "721/721 [==============================] - 57s 79ms/step - loss: 0.7274 - accuracy: 0.7236 - val_loss: 1.3886 - val_accuracy: 0.5584\n",
      "Epoch 17/20\n",
      "721/721 [==============================] - 49s 68ms/step - loss: 0.6903 - accuracy: 0.7389 - val_loss: 1.4112 - val_accuracy: 0.5596\n",
      "Epoch 18/20\n",
      "721/721 [==============================] - 49s 67ms/step - loss: 0.6560 - accuracy: 0.7516 - val_loss: 1.4464 - val_accuracy: 0.5500\n",
      "Epoch 19/20\n",
      "721/721 [==============================] - 49s 68ms/step - loss: 0.6259 - accuracy: 0.7623 - val_loss: 1.5193 - val_accuracy: 0.5658\n",
      "Epoch 20/20\n",
      "721/721 [==============================] - 49s 68ms/step - loss: 0.6032 - accuracy: 0.7681 - val_loss: 1.5593 - val_accuracy: 0.5603\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23205b978b0>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entrenar el modelo\n",
    "model.fit(train_images, train_labels, epochs=30, validation_data=(val_images, val_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1004b048-3c69-41f2-8d1f-94333c55f99f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221/221 [==============================] - 3s 12ms/step - loss: 339.0875 - accuracy: 0.4196\n",
      "Test accuracy: 0.4196150600910187\n"
     ]
    }
   ],
   "source": [
    "# Cargar los datos de prueba\n",
    "test_dir = 'DataSet/images/validation/'\n",
    "test_images = []\n",
    "test_labels = []\n",
    "for emotion in emotions:\n",
    "    emotion_dir = os.path.join(test_dir, emotion)\n",
    "    for filename in os.listdir(emotion_dir):\n",
    "        if filename.endswith('.jpg'):\n",
    "            img = cv2.imread(os.path.join(emotion_dir, filename), cv2.IMREAD_GRAYSCALE)\n",
    "            img = cv2.resize(img, img_size)\n",
    "            test_images.append(img)\n",
    "            test_labels.append(emotions.index(emotion))\n",
    "test_images = np.array(test_images)\n",
    "test_labels = np.array(test_labels)\n",
    "\n",
    "# Evaluar el modelo en los datos de prueba\n",
    "loss, accuracy = model.evaluate(test_images, test_labels)\n",
    "print('Test accuracy:', accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d9dd5b0-24db-4aeb-9716-deb67d30fc5c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 130ms/step\n",
      "Predicted emotion: surprise\n"
     ]
    }
   ],
   "source": [
    "# Cargar una imagen de cara nueva\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "# Tamaño de las imágenes de entrada\n",
    "img_size = (48, 48)\n",
    "\n",
    "# Lista de emociones disponibles\n",
    "emotions = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n",
    "new_img = cv2.imread('face0.png', cv2.IMREAD_GRAYSCALE)\n",
    "new_img = cv2.resize(new_img, img_size)\n",
    "\n",
    "# Hacer una predicción sobre la emoción de la cara\n",
    "prediction = model.predict(np.array([new_img]))\n",
    "predicted_emotion = emotions[np.argmax(prediction)]\n",
    "print('Predicted emotion:', predicted_emotion)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2621f86e-4744-4291-b438-e2a6d531373b",
   "metadata": {
    "tags": []
   },
   "source": [
    "model.save('my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24d5ce07-fe9a-42c5-8bd2-3bff7769ba52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "model = load_model('my_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "989f01c7-f4de-45b3-aeb2-b4574a12e3ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Cargar el modelo\n",
    "model = load_model('my_model.h5')\n",
    "\n",
    "# Definir las emociones y colores correspondientes\n",
    "emotions = ['angry', 'disgusted', 'fearful', 'happy', 'neutral', 'sad', 'surprised']\n",
    "colors = [(255,0,0), (0,255,0), (0,0,255), (255,255,0), (255,255,255), (0,0,0), (255,0,255)]\n",
    "\n",
    "# Cargar el clasificador de Haar Cascade para la detección de rostros\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Usar la cámara predeterminada\n",
    "cap = cv2.VideoCapture(0) \n",
    "\n",
    "while True:\n",
    "    # Capturar el siguiente cuadro del video\n",
    "    ret, frame = cap.read() \n",
    "    if not ret:\n",
    "        break\n",
    "        \n",
    "    # Convertir a escala de grises\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) \n",
    "    \n",
    "    # Detectar rostros en la imagen\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "    \n",
    "    # Recortar la región de interés (ROI) de la imagen original\n",
    "    for (x,y,w,h) in faces:\n",
    "        face_img = gray[y:y+h, x:x+w]\n",
    "        face_img = cv2.resize(face_img, (48, 48)) # Redimensionar a 48x48 (tamaño de entrada del modelo)\n",
    "        prediction = model.predict(np.array([face_img]))\n",
    "        predicted_emotion = emotions[np.argmax(prediction)]\n",
    "        color = colors[np.argmax(prediction)]\n",
    "        cv2.putText(frame, predicted_emotion, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2)\n",
    "        cv2.rectangle(frame,(x,y),(x+w,y+h),color,2)\n",
    "    \n",
    "    cv2.imshow('Emotion Detection', frame) # Mostrar el cuadro con la emoción detectada\n",
    "    \n",
    "    # Salir con la tecla 'q'\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'): \n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "af3a0ff3-4378-45c7-8e5f-da5820cef7c1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 72   5 179 179]]\n",
      "WARNING:tensorflow:5 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001CCCD89CCA0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "Predicted emotion: happy\n"
     ]
    }
   ],
   "source": [
    "# Cargar una imagen de cara nueva\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = load_model('my_model.h5')\n",
    "\n",
    "\n",
    "# Load the cascade\n",
    "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_alt.xml')\n",
    "\n",
    "img = cv2.imread('Images/Triste.jpg')\n",
    "img_height, img_width, _ = img.shape\n",
    "aspect_ratio = img_width / img_height\n",
    "new_height = 256\n",
    "new_width = int(new_height * aspect_ratio)\n",
    "img_resized = cv2.resize(img, (new_width, new_height))\n",
    "gray = cv2.cvtColor(img_resized, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "faces = face_cascade.detectMultiScale(gray, 1.06, 4)\n",
    "print(faces)\n",
    "\n",
    "\n",
    "i = 0\n",
    "# Crear una copia de la imagen original\n",
    "img_copy = img_resized.copy()\n",
    "for (x, y, w, h) in faces:\n",
    "    # Dibujar el rectángulo en la copia\n",
    "    cv2.rectangle(img_copy, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
    "    # Ampliar la ROI\n",
    "    roi_x = max(x - int(w*0.1), 0)\n",
    "    roi_y = max(y - int(h*0.1), 0)\n",
    "    roi_w = min(int(w*1.2), img_resized.shape[1]-roi_x)\n",
    "    roi_h = min(int(h*1.2), img_resized.shape[0]-roi_y)\n",
    "    # Extraer la ROI de la imagen original\n",
    "    face_img = img_resized[roi_y:roi_y+roi_h, roi_x:roi_x+roi_w]\n",
    "    # Guardar la imagen de la cara sin el rectángulo\n",
    "    cv2.imwrite(f'face{i}.png', face_img)\n",
    "    # Incrementar el contador\n",
    "    i += 1\n",
    "    \n",
    "# Tamaño de las imágenes de entrada\n",
    "img_size = (48, 48)\n",
    "\n",
    "\n",
    "\n",
    "for foto in range(i):\n",
    "    filename = f\"face{i}.png\"\n",
    "    if os.path.isfile(filename):\n",
    "        # Lista de emociones disponibles\n",
    "        emotions = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n",
    "        new_img = cv2.imread(filename, cv2.IMREAD_GRAYSCALE)\n",
    "        new_img = cv2.resize(new_img, img_size)\n",
    "\n",
    "        # Hacer una predicción sobre la emoción de la cara\n",
    "        prediction = model.predict(np.array([new_img]))\n",
    "        predicted_emotion = emotions[np.argmax(prediction)]\n",
    "        print('Predicted emotion:', predicted_emotion)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
